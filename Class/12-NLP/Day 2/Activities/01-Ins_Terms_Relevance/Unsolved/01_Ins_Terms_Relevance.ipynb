{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Do: Terms Relevance (Understanding TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Text from the Reuters Dataset\n",
    "\n",
    "To demonstrate how TF-IDF works, we will use the _Reuters_ dataset that is bundled in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download/update the Reuters dataset\n",
    "nltk.download(\"reuters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs in the corpus: 10788\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of documents in the collection\n",
    "doc_ids = reuters.fileids()\n",
    "print(f\"Total number of docs in the corpus: {len(doc_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Bag of Words from a Single Document\n",
    "\n",
    "We select a single document from the corpus to get it's \"Bag of Words\". The same can be done from multiple documents by pasing a list of documents (or documents ids on this example) to the `CountVectorizer()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH ADJUSTED UNEMPLOYMENT RISES IN MARCH\n",
      "  Dutch seasonally adjusted unemployment\n",
      "  rose in the month to end-March to a total 693,000 from 690,600\n",
      "  at end-February, but was well down from 730,100 at end-March\n",
      "  1986, Social Affairs Ministry figures show.\n",
      "      The figure for male jobless rose by 2,000 in the month to\n",
      "  436,500 compared with 470,700 a year earlier. The figure for\n",
      "  women was 256,500 at end-March against 256,100 a month earlier\n",
      "  and 259,400 at end-March 1986.\n",
      "      On an unadjusted basis total unemployment fell by 16,500 in\n",
      "  the month to end-March to 692,200. In March 1986 the figure was\n",
      "  725,000.\n",
      "      A ministry spokesman said the unadjusted figures showed a\n",
      "  smaller than usual seasonal decrease for the time of year,\n",
      "  because of particularly cold weather delaying work in the\n",
      "  building industry. He said this explained the increase in the\n",
      "  adjusted statistics.\n",
      "      Total vacancies available rose by 1,900 to 26,300 at\n",
      "  end-March. A year earlier the figure was 28,763.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select and print the original single document text\n",
    "doc_id = \"test/15045\"\n",
    "doc_text = reuters.raw(doc_id)\n",
    "print(doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '100', '16', '1986', '200', '256', '259', '26', '28', '300', '400', '436', '470', '500', '600', '690', '692', '693', '700', '725', '730', '763', '900', 'adjusted', 'affairs', 'available', 'basis', 'building', 'cold', 'compared', 'decrease', 'delaying', 'dutch', 'earlier', 'end', 'explained', 'february', 'fell', 'figure', 'figures', 'increase', 'industry', 'jobless', 'male', 'march', 'ministry', 'month', 'particularly', 'rises', 'rose', 'said', 'seasonal', 'seasonally', 'showed', 'smaller', 'social', 'spokesman', 'statistics', 'time', 'total', 'unadjusted', 'unemployment', 'usual', 'vacancies', 'weather', 'women', 'work', 'year']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffr\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Creating the CountVectorizer instance defining the stop words in English to be ignored\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Getting the tokenization and occurrence counting\n",
    "X = vectorizer.fit_transform([doc_text])\n",
    "\n",
    "# Retrieve unique words list\n",
    "words = vectorizer.get_feature_names()\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 32)\t2\n",
      "  (0, 23)\t3\n",
      "  (0, 61)\t3\n",
      "  (0, 48)\t1\n",
      "  (0, 44)\t8\n",
      "  (0, 52)\t1\n",
      "  (0, 49)\t3\n",
      "  (0, 46)\t4\n",
      "  (0, 34)\t7\n",
      "  (0, 59)\t3\n",
      "  (0, 17)\t1\n",
      "  (0, 0)\t3\n",
      "  (0, 15)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 3)\t3\n",
      "  (0, 55)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 45)\t2\n",
      "  (0, 39)\t2\n",
      "  (0, 38)\t4\n",
      "  (0, 43)\t1\n",
      "  (0, 42)\t1\n",
      "  :\t:\n",
      "  (0, 56)\t1\n",
      "  (0, 50)\t2\n",
      "  (0, 53)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 62)\t1\n",
      "  (0, 51)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 66)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "# X raw data contains the occurrence of each term in the document. A unique ID is assigned to each term.\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the bag of words as DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the TF-IDF from a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the corpus (first 1000 files from Reuters dataset)\n",
    "\n",
    "\n",
    "# Print sample document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting matrix info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve words list from corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the TF-IDF weight of each word in corpus as DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest 10 TF-IDF scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest 10 TF-IDF scores\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
